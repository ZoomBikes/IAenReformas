# ü§ñ Arquitectura de IA: Optimizaci√≥n de Tokens

## üìä Estrategia de Minimizaci√≥n de Tokens

### **Problema Actual**
Sin optimizaci√≥n: Cada presupuesto podr√≠a consumir ~2000-5000 tokens
Con optimizaci√≥n: Reducir a ~500-1000 tokens por presupuesto

### **Ahorro Objetivo: 70-80%**

---

## üéØ Sistema de Prompts Modulares

### **Nivel 1: Plantillas Pre-definidas (0 tokens)**

```typescript
const PLANTILLAS = {
  tarima: {
    basica: "Instalaci√≥n de tarima flotante b√°sica en {metros} m¬≤. Incluye preparaci√≥n del suelo, autonivelado y rodapi√©s.",
    estandar: "Instalaci√≥n de tarima flotante de calidad est√°ndar en {metros} m¬≤. Preparaci√≥n completa del suelo, autonivelado profesional y rodapi√©s de madera.",
    premium: "Instalaci√≥n de tarima de alta calidad en {metros} m¬≤. Preparaci√≥n exhaustiva del suelo, autonivelado de precisi√≥n y rodapi√©s premium."
  },
  pintura: {
    conAlisado: "Pintura de paredes en {metros} m¬≤. Incluye alisado previo, imprimaci√≥n y aplicaci√≥n de pintura pl√°stica en {manos} manos.",
    sinAlisado: "Pintura directa sobre {metros} m¬≤. Imprimaci√≥n y aplicaci√≥n de pintura pl√°stica en {manos} manos."
  }
}

// Uso: 0 tokens de OpenAI
function generarExplicacion(trabajo: Trabajo) {
  const template = PLANTILLAS[trabajo.tipo]?.[trabajo.calidad];
  if (template) {
    return template
      .replace('{metros}', trabajo.cantidad)
      .replace('{manos}', trabajo.numManos || 2);
  }
  return null; // Si no hay template, usar IA
}
```

### **Nivel 2: Function Calling (Tokens m√≠nimos)**

Para trabajos complejos, usar function calling:

```typescript
const funcionesIA = [
  {
    name: "generar_explicacion_breve",
    description: "Genera una explicaci√≥n de 2-3 frases para un trabajo de reforma",
    parameters: {
      type: "object",
      properties: {
        trabajo: {
          type: "string",
          enum: ["tarima", "pintura", "azulejos", "fontaneria", ...]
        },
        metros: { type: "number" },
        calidad: {
          type: "string",
          enum: ["basica", "estandar", "premium"]
        },
        explicacion: {
          type: "string",
          description: "M√°ximo 150 palabras"
        }
      },
      required: ["trabajo", "metros", "calidad", "explicacion"]
    }
  }
]

// Prompt ultra-condensado
const prompt = `Genera explicaci√≥n breve para: ${tipoTrabajo}, ${metros}m¬≤, calidad ${calidad}`;
// ‚âà 20 tokens input
// ‚âà 50 tokens output
// Total: ~70 tokens por trabajo
```

### **Nivel 3: Batch Processing**

Agrupar m√∫ltiples trabajos en una sola llamada:

```typescript
// ‚ùå MAL: 5 llamadas separadas
for (const trabajo of trabajos) {
  await openai.chat.completions.create({
    messages: [{ role: "user", content: `Explica: ${trabajo}` }]
  });
}
// Total: ~350 tokens

// ‚úÖ BIEN: 1 llamada con array
const trabajosData = trabajos.map(t => ({
  tipo: t.tipo,
  metros: t.cantidad,
  calidad: t.calidad
}));

await openai.chat.completions.create({
  messages: [{
    role: "user",
    content: `Genera explicaciones breves para estos trabajos: ${JSON.stringify(trabajosData)}`
  }],
  functions: funcionesIA,
  max_tokens: 300 // Limitar respuesta
});
// Total: ~150 tokens (ahorro de 57%)
```

---

## üíæ Sistema de Cach√© Inteligente

### **Estructura de Cach√©**

```typescript
interface CacheExplicacion {
  hash: string; // Hash de: tipo+metros+calidad+condiciones
  texto: string;
  fechaCreacion: Date;
  usos: number;
}

// Generar hash √∫nico
function generarHash(trabajo: Trabajo): string {
  const key = `${trabajo.tipo}-${trabajo.cantidad}-${trabajo.calidad}-${trabajo.condiciones.join(',')}`;
  return crypto.createHash('sha256').update(key).digest('hex').substring(0, 16);
}

// Verificar cach√© antes de llamar a IA
async function obtenerExplicacion(trabajo: Trabajo): Promise<string> {
  const hash = generarHash(trabajo);
  
  // 1. Buscar en cach√© local (Redis/DB)
  const cached = await db.cacheExplicacion.findFirst({
    where: { hash }
  });
  
  if (cached) {
    // Actualizar contador
    await db.cacheExplicacion.update({
      where: { id: cached.id },
      data: { usos: cached.usos + 1 }
    });
    return cached.texto; // 0 tokens
  }
  
  // 2. Si no est√° en cach√©, usar plantilla
  const plantilla = PLANTILLAS[trabajo.tipo]?.[trabajo.calidad];
  if (plantilla) {
    const texto = aplicarTemplate(plantilla, trabajo);
    // Guardar en cach√© para futuros usos similares
    await db.cacheExplicacion.create({
      data: { hash, texto, usos: 1 }
    });
    return texto; // 0 tokens
  }
  
  // 3. √öltimo recurso: llamar a IA
  const texto = await generarConIA(trabajo);
  
  // Guardar en cach√©
  await db.cacheExplicacion.create({
    data: { hash, texto, usos: 1 }
  });
  
  return texto; // ‚âà70 tokens
}
```

### **Estrategia de Invalidez de Cach√©**

```typescript
// Invalidar cach√© despu√©s de X d√≠as o si hay actualizaci√≥n de precios
const CACHE_TTL = 30 * 24 * 60 * 60 * 1000; // 30 d√≠as

// Limpiar cach√© antiguo peri√≥dicamente
async function limpiarCacheAntiguo() {
  await db.cacheExplicacion.deleteMany({
    where: {
      fechaCreacion: {
        lt: new Date(Date.now() - CACHE_TTL)
      },
      usos: { lt: 5 } // Solo eliminar si se us√≥ menos de 5 veces
    }
  });
}
```

---

## üìù Prompt Engineering Optimizado

### **Prompts Ultra-Condensados**

```typescript
// ‚ùå MAL (verboso, muchos tokens)
const promptMal = `
Por favor, genera una explicaci√≥n detallada y profesional para el cliente sobre el siguiente trabajo de reforma:

Tipo de trabajo: Cambio de tarima
Metros cuadrados: 50
Calidad: Est√°ndar
Ubicaci√≥n: Sal√≥n principal

La explicaci√≥n debe ser clara, profesional y detallar todos los pasos del proceso...
`;
// ~150 tokens

// ‚úÖ BIEN (condensado, espec√≠fico)
const promptBien = `Expl: tarima 50m¬≤ est√°ndar. Max 100 palabras.`;
// ~15 tokens

// Con function calling es a√∫n mejor:
const promptOptimizado = {
  role: "user",
  content: "tarima 50m¬≤ est√°ndar",
  functions: [funcionGenerarExplicacion]
};
// ~10 tokens
```

### **Template de Resumen del Proyecto**

```typescript
// Resumen general del presupuesto (una sola llamada)
async function generarResumenProyecto(presupuesto: Presupuesto): Promise<string> {
  // Extraer solo informaci√≥n esencial
  const datos = {
    tipo: presupuesto.tipoObra,
    metros: presupuesto.metrosTotales,
    trabajos: presupuesto.trabajos.map(t => t.tipoTrabajo),
    total: presupuesto.total
  };
  
  const prompt = `Resumen proyecto: ${JSON.stringify(datos)}. 3 p√°rrafos m√°ximo.`;
  
  const response = await openai.chat.completions.create({
    model: "gpt-4-turbo-preview", // M√°s eficiente
    messages: [{ role: "user", content: prompt }],
    max_tokens: 200, // Limitar estrictamente
    temperature: 0.7 // Consistente pero no demasiado creativo
  });
  
  return response.choices[0].message.content;
}
```

---

## üîÑ Flujo Completo Optimizado

```typescript
async function generarPresupuestoConIA(presupuesto: Presupuesto) {
  const explicaciones: string[] = [];
  let tokensUsados = 0;
  
  // 1. Generar explicaciones de trabajos (batch)
  const trabajosParaIA = [];
  for (const trabajo of presupuesto.trabajos) {
    // Intentar cach√©/plantilla primero
    let explicacion = await obtenerExplicacion(trabajo);
    
    if (!explicacion) {
      // Agregar a batch para procesar juntos
      trabajosParaIA.push(trabajo);
    } else {
      explicaciones.push(explicacion);
    }
  }
  
  // 2. Si hay trabajos sin explicaci√≥n, procesar en batch
  if (trabajosParaIA.length > 0) {
    const batchResult = await generarExplicacionesBatch(trabajosParaIA);
    explicaciones.push(...batchResult.explicaciones);
    tokensUsados += batchResult.tokens;
  }
  
  // 3. Generar resumen del proyecto (solo una llamada)
  const resumen = await generarResumenProyecto(presupuesto);
  tokensUsados += 150; // Estimado
  
  // 4. Total estimado por presupuesto
  // - 5 trabajos √ó 70 tokens = 350 (si ninguno en cach√©)
  // - Resumen = 150 tokens
  // Total: ~500 tokens (vs ~2500 sin optimizaci√≥n)
  
  return {
    explicaciones,
    resumen,
    tokensUsados,
    ahorro: calcularAhorro(presupuesto.trabajos.length)
  };
}
```

---

## üìä M√©tricas y Monitoreo

```typescript
interface MetricasIA {
  presupuestoId: string;
  tokensUsados: number;
  tokensAhorrados: number; // Por cach√©/plantillas
  trabajosEnCache: number;
  trabajosConIA: number;
  tiempoGeneracion: number; // ms
  costoEstimado: number; // USD
}

// Tracking para optimizar
async function trackearUsoIA(metricas: MetricasIA) {
  await db.metricasIA.create({ data: metricas });
  
  // Alertar si consumo es alto
  if (metricas.tokensUsados > 1000) {
    console.warn(`Alto consumo de tokens en presupuesto ${metricas.presupuestoId}`);
  }
}
```

---

## üí∞ C√°lculo de Costos

```
Precios OpenAI (GPT-4 Turbo):
- Input: $0.01 / 1K tokens
- Output: $0.03 / 1K tokens

Presupuesto promedio optimizado:
- Input: ~200 tokens = $0.002
- Output: ~300 tokens = $0.009
- Total: ~$0.011 por presupuesto

Sin optimizaci√≥n:
- Input: ~800 tokens = $0.008
- Output: ~1200 tokens = $0.036
- Total: ~$0.044 por presupuesto

Ahorro: 75% de costos
```

---

## ‚úÖ Checklist de Optimizaci√≥n

- [x] Plantillas pre-definidas para trabajos comunes
- [x] Sistema de cach√© con hash √∫nico
- [x] Batch processing para m√∫ltiples trabajos
- [x] Function calling para respuestas estructuradas
- [x] Prompts ultra-condensados
- [x] L√≠mites estrictos de max_tokens
- [x] Modelo eficiente (gpt-4-turbo-preview)
- [x] Monitoreo de uso de tokens
- [ ] A/B testing de prompts
- [ ] Machine learning para mejorar plantillas

